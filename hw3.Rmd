---
title: "Hw3"
author: "Lianghui Li"
date: "9/18/2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(knitr)
library(ggplot2)
```

## 3.3

### a

We know under gamma prior $G(a,b)$ and $y_i \mid \theta$ follows poisson, the posterior will be $G(a + y,b + n)$, where $y$ is number of the observed, $n$ is number of events.

The posterior distribution for A is $\theta_A \mid y_A$ ~ $gamma(120 + \sum^{10} y_{A_i} = 237, 10 + 10 = 20)$, The posterior distribution for B is $\theta_B \mid y_B$ ~ $gamma(12 + \sum^{10} y_{B_i} = 125, 1 + 13 = 14)$.


```{r}
y_a = c(12,9,12,14,13,13,15,8,15,6)
y_b = c(11,11,10,9,9,8,7,10,6,8,8,9,7)

a_a  = 120
b_a = 10

a_b = 12
b_b = 1
 
a_a_post = a_a + sum(y_a)
b_a_post =  b_a + length(y_a)

a_b_post = a_b + sum(y_b)
b_b_post = b_b + length(y_b)

a_mean_post = a_a_post / b_a_post
b_mean_post = a_b_post / b_b_post

a_var_post = a_a_post / b_a_post^2
b_var_post = a_b_post / b_b_post^2

ci_a_post = qgamma(c(0.025,0.975), a_a_post, b_a_post)
ci_b_post = qgamma(c(0.025,0.975), a_b_post, b_b_post)

df1 = data.frame(t(c(a_mean_post, a_var_post, ci_a_post)))
df1 = rbind(df1, c(b_mean_post, b_var_post, ci_b_post))
colnames(df1) = c("mean", "variance", "2.5%", "97.5%")
rownames(df1) = c("a", "b")

kable(df1)

```


###b

```{r}
n0 = seq(50)
a0 = 12 *n0
b0 = n0 

a_b_post = a0 + sum(y_b)
b_b_post = b0 + length(y_b)

mean_b_post = a_b_post /b_b_post

df2 = data.frame(n0, mean_b_post)

ggplot(df2, aes(x = n0, y = mean_b_post )) + geom_line()


```

From part a), we know that posterior expectation of $\theta_A$ is 11.85, in order to know what value $n_0$ is necessary for $\theta_B$ to achieve the same value, we can solve the equation $11.85 = \frac{12n_0 + 113}{n_0 +13}$, and $n_0$ should be at least 274.

###c

Given from the problem, we know that Type B mice are related to type A mice, which means they are not indepedent. with the equation $p(\theta_A, \theta_b) = p(\theta_A)p(\theta_B)$, that implies A and B are independent in terms of our prior belief; therefore, it does not make sense to have to equation above.


##4.1

Since prior for $\theta_2$ is $Beta(1,1)$ and the sampling is $Bino(50,30)$, so the posterior for $\theta_2$ is $Beta(31,21)$.

```{r}
theta = seq(0,1, by = 0.1)

y1 = 57
n1 = 100
y2 = 30
n2 = 50

p_y_theta <- dbeta(theta, y2 + 1, 1 + n2 - y2)
ptheta <- rep(1/length(theta), length(theta))
p_joint <- p_y_theta*ptheta
p_post <- p_joint/sum(p_joint)

theta2_mc <- sample(x = theta, 
                           size = 5000, 
                           replace = TRUE, 
                           prob = p_post)


p_y_theta1 = dbeta(theta, 1 + y1, 1+ n1 -y1)

p_joint1 <- p_y_theta1*ptheta
p_post1 <- p_joint1/sum(p_joint1)

theta1_mc <- sample(x = theta, 
                           size = 5000, 
                           replace = TRUE, 
                           prob = p_post1)

mean(theta1_mc < theta2_mc)

```

Therefore, $E(Pr(\theta_1 < \theta_2 \mid  \text{data & prior})) = 0.3734$


##4.2

###a 

```{r}
#rerun the code from the first question
y_a = c(12,9,12,14,13,13,15,8,15,6)
y_b = c(11,11,10,9,9,8,7,10,6,8,8,9,7)

a_a  = 120
b_a = 10

a_b = 12
b_b = 1
 
a_a_post = a_a + sum(y_a)
b_a_post =  b_a + length(y_a)

a_b_post = a_b + sum(y_b)
b_b_post = b_b + length(y_b)

s = 5000
set.seed(123)

a_sample = rgamma(s, a_a_post, b_a_post)
b_sample = rgamma(s, a_b_post, b_b_post)

mean(a_sample > b_sample)

```

Therefore, $Pr(\theta_b < \theta_a \mid y_a, y_b) \approx 0.99$


###b

```{r}
n0 = seq(50)
a0 = 12 *n0
b0 = n0 

a_b_post = a0 + sum(y_b)
b_b_post = b0 + length(y_b)

prob = rep(NA, length(n0))

set.seed(123)

for(i in seq_along(n0)) {
  theta1 = rgamma(s,a_a_post, b_a_post)
  theta2 = rgamma(s,a_b_post[i], b_b_post[i])
  prob[i] = mean(theta1 > theta2)
}

df3 = data_frame(n0, prob)

ggplot(df3, aes(x= n0, y = prob)) + geom_line()

```

From the graph, we can clearly see the decreasing trend, which implies the more increase in the prior sample size by increasing $n_0$, the less likely $\theta_B < \theta_A$. One thing to notice, when $n_0 = 50$, the maximum prior sample size, the probability is still around 0.7, so the conclusion on $\theta_B < \theta_A$ is not senstive to the prior distribution on $\theta_B$.

###c

With the prior is gamma(a,b) and sampling distribution is poisson($\theta$), the predictive distribution will be negative binomial(r,p), where $r= a + \sum y_i$ and $p = \frac{1}{b+n+1}$, but unfortunately in r, we need to do that in two steps

```{r}
prob1 = rep(NA, length(n0))

set.seed(123)
for(i  in seq_along(n0)) {
  theta_1 = rgamma(s, a_a_post, b_a_post )
  predy_1 = rpois(s, theta_1)
  theta_2 = rgamma(s, a_b_post[i], b_b_post[i])
  predy_2 = rpois(s, theta_2)
  prob1[i] = mean(predy_2 < predy_1)
}

df4 = data.frame(n0, prob1)


ggplot(data = df4, aes(n0, prob1)) + geom_line()
```

The downward sloping trend means increasing the $n_0$ will decrease the probability of $\widetilde{y_B} < \widetilde{y_A}$; and this time, the predictive value are more senstive on the prior sample size than the posterior mean. Moreover, if we increase $n_0$ even more, it is possible to have $Pr(\widetilde{y_B} < \widetilde{y_A}) < 0.5$


##Math problem


In order to satisfy conjugacy, it should satisfy: $$p(\theta) \in P \Rightarrow p(\theta \mid y) \in P$$

With the sampling distrbution is geometric, $Pr(Y = k \mid p) = (1 - p)^{k -1} p$
our educated guess for its conjugate will be beta, because they share the similar form, then posterior will be beta as well. Assume Beta(a,b) is prior, the posterior will be Beta(a, b+k).

"a" would approximately be prior number of 1.
"b" would approximately be prior number of 0.
"a+b" would approximately be prior sample size.